{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82cf3cbd-856f-4a77-934a-05ae47c1cbff",
   "metadata": {},
   "source": [
    "### Modifications from Original\n",
    "- Changed LLModuleConfig to take in registry, bug in README (Have not fixed README, however)\n",
    "- Created custom hallucination detection signatures, and operators\n",
    "- Fixed some refactorings and forgotten imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b6837d-b7b3-42e2-8e9a-4c8e3d0b5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# 1) Import our dataset registry tools:\n",
    "from src.avior.registry.dataset.registry.metadata_registry import DatasetMetadataRegistry\n",
    "from src.avior.registry.dataset.registry.loader_factory import DatasetLoaderFactory\n",
    "from src.avior.registry.dataset.registry.initialization import initialize_dataset_registry\n",
    "\n",
    "# 2) Import or define dataset loader/validator/sampler:\n",
    "# If you have existing ones, import them. For now, we'll assume defaults or mocks.\n",
    "from src.avior.registry.dataset.base.loaders import HuggingFaceDatasetLoader, IDatasetLoader\n",
    "from src.avior.registry.dataset.base.validators import IDatasetValidator\n",
    "from src.avior.registry.dataset.base.samplers import IDatasetSampler\n",
    "from src.avior.registry.dataset.base.models import DatasetInfo, DatasetEntry, TaskType\n",
    "from src.avior.registry.dataset.base.preppers import IDatasetPrepper\n",
    "from src.avior.registry.dataset.datasets.mmlu import MMLUConfig\n",
    "from src.avior.registry.dataset.base.validators import DatasetValidator\n",
    "from src.avior.registry.dataset.base.samplers import DatasetSampler\n",
    "from src.avior.registry.dataset.datasets.halueval import HaluEvalConfig\n",
    "\n",
    "# 3) Import the DatasetService to actually use the pipeline:\n",
    "from src.avior.registry.dataset.registry.service import DatasetService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a30e888-0392-40d4-855b-27edbc40f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.avior.registry.model.registry.model_registry import ModelRegistry\n",
    "from src.avior.registry.model.schemas.model_info import ModelInfo\n",
    "from src.avior.registry.model.schemas.provider_info import ProviderInfo\n",
    "from src.avior.registry.model.schemas.cost import ModelCost, RateLimit\n",
    "from src.avior.registry.model.services.usage_service import UsageService\n",
    "from src.avior.registry.model.services.model_service import ModelService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7467021b-93a5-45ac-875c-ab69e50a3c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.avior.registry.model.config import initialize_global_registry, GLOBAL_MODEL_REGISTRY\n",
    "from src.avior.registry.model.services.model_service import ModelService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff37b94-fed6-4064-888c-fc86fad39b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.avior.registry.model.registry.model_enum import OpenAIModelEnum as OME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5911865c-9511-435f-a210-65026b4c9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.avior.registry.model.config import AviorSettings\n",
    "\n",
    "settings = AviorSettings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b107a14-7376-4a52-95e0-0d74052ad769",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2d967b1-c2d9-4f09-81e2-5cec5830d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Create a metadata registry and loader factory:\n",
    "metadata_registry = DatasetMetadataRegistry()\n",
    "loader_factory = DatasetLoaderFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db1b5879-85a2-4a9e-ba93-a2d48d0019bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: truthful_qa\n",
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: mmlu\n",
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: commonsense_qa\n",
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: halueval\n",
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: my_shortanswer_ds\n",
      "INFO:src.avior.registry.dataset.registry.loader_factory:Registered loader prepper for dataset: my_code_ds\n",
      "INFO:src.avior.registry.dataset.registry.initialization:Initialized dataset registry with known datasets.\n"
     ]
    }
   ],
   "source": [
    "# 2) Initialize the registry with known “built-in” datasets:\n",
    "initialize_dataset_registry(metadata_registry, loader_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac165ec9-43c7-46b6-bfab-098127230c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.avior.registry.dataset.registry.loader_factory:Auto-registered plugin preppers: []\n"
     ]
    }
   ],
   "source": [
    "# 3) Optionally, discover any additional plugin-based preppers from pyproject.toml:\n",
    "loader_factory.discover_and_register_plugins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7220f8f7-6a17-41cc-84f8-af8557aaa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cursor Suggestion\n",
    "\n",
    "# 6) Construct a dataset loader, validator, and sampler:\n",
    "loader: IDatasetLoader = HuggingFaceDatasetLoader()\n",
    "validator: IDatasetValidator = DatasetValidator()\n",
    "sampler: IDatasetSampler = DatasetSampler()\n",
    "\n",
    "# 7) Instantiate a DatasetService to handle load, validation, transform, sampling, and prep:\n",
    "dataset_service = DatasetService(\n",
    "    loader=loader,\n",
    "    validator=validator,\n",
    "    sampler=sampler,\n",
    "    transformers=[]  # Insert any specialized transformers if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3aca721-cb5b-40f3-8095-014db2d2003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/kunalagrawal/anaconda3/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b05a932c-ef2b-460f-aeb2-e9d7dea8bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using pydantic-based config or environment variables\n",
    "openai_key = settings.openai_api_key or os.getenv(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ae461a-95ec-429c-9b02-afff8b45b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalagrawal/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_id\" in LMModuleConfig has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.avior.registry.model.registry.model_registry import ModelRegistry\n",
    "from src.avior.registry.operator.operator_registry import EnsembleOperator, GetAnswerOperator\n",
    "from src.avior.registry.operator.operator_base import LMModuleConfig, LMModule\n",
    "\n",
    "# 1) Register the models\n",
    "registry = ModelRegistry()\n",
    "# (Imagine we've done registry.register_model(...) for each: gemini, claude, gpt-4o, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34e45f1-942e-471a-a8bc-6169f5287ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register each model\n",
    "openai_provider = ProviderInfo(\n",
    "    name=\"OpenAI\", \n",
    "    default_api_key=openai_key,\n",
    "    base_url=\"https://api.openai.com\"\n",
    ")\n",
    "\n",
    "# GPT-4o\n",
    "gpt4o_info = ModelInfo(\n",
    "    model_id=\"openai:gpt-4o\",\n",
    "    model_name=\"gpt-4o\",\n",
    "    cost=ModelCost(\n",
    "        input_cost_per_million=5000,   # $5.00 per million input tokens\n",
    "        output_cost_per_million=15000  # $15.00 per million output tokens\n",
    "    ),\n",
    "    rate_limit=RateLimit(\n",
    "        tokens_per_minute=10000000,    # 10M tokens per minute\n",
    "        requests_per_minute=1500       # Tier 5 rate limit\n",
    "    ),\n",
    "    provider=openai_provider,\n",
    "    api_key=openai_key\n",
    ")\n",
    "\n",
    "# GPT-4o-mini\n",
    "gpt4o_mini_info = ModelInfo(\n",
    "    model_id=\"openai:gpt-4o-mini\",\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    cost=ModelCost(\n",
    "        input_cost_per_million=150,    # $0.15 per million input tokens\n",
    "        output_cost_per_million=600    # $0.60 per million output tokens\n",
    "    ),\n",
    "    rate_limit=RateLimit(\n",
    "        tokens_per_minute=10000000,\n",
    "        requests_per_minute=1500\n",
    "    ),\n",
    "    provider=openai_provider,\n",
    "    api_key=openai_key\n",
    ")\n",
    "\n",
    "# O1\n",
    "o1_info = ModelInfo(\n",
    "    model_id=\"openai:o1\",\n",
    "    model_name=\"o1\",\n",
    "    cost=ModelCost(\n",
    "        input_cost_per_million=10000,  # $10.00 per million input tokens\n",
    "        output_cost_per_million=20000  # $20.00 per million output tokens\n",
    "    ),\n",
    "    rate_limit=RateLimit(\n",
    "        tokens_per_minute=5000000,\n",
    "        requests_per_minute=1000\n",
    "    ),\n",
    "    provider=openai_provider,\n",
    "    api_key=openai_key\n",
    ")\n",
    "\n",
    "# Register all models\n",
    "registry.register_model(gpt4o_info)\n",
    "registry.register_model(gpt4o_mini_info)\n",
    "registry.register_model(o1_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92beba25-6948-4dd5-95ed-1456cef17b5e",
   "metadata": {},
   "source": [
    "#### README Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eb3612a-a49a-4545-8985-6cbc3e4a30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a usage service to use for the model service\n",
    "usage_service = UsageService()\n",
    "\n",
    "# Create a model service using the registry\n",
    "model_service = ModelService(registry=registry, usage_service=usage_service)\n",
    "\n",
    "# 2) Create LMModules for each\n",
    "# gemini_mod = LMModule(LMModuleConfig(model_name=\"gemini-1.5-pro\"), model_service) #TODO might be bug in README, registry instead of model_service\n",
    "# claude_mod = LMModule(LMModuleConfig(model_name=\"claude-3.5-sonnet\"), model_service)\n",
    "# Just using OpenAI models for now\n",
    "g4o_mod = LMModule(LMModuleConfig(model_name=\"gpt-4o\"), model_service)\n",
    "g4omini_mod = LMModule(LMModuleConfig(model_name=\"gpt-4o-mini\"), model_service)\n",
    "go1_mod = LMModule(LMModuleConfig(model_name=\"o1\"), model_service)\n",
    "\n",
    "# 3) Instantiate an EnsembleOperator\n",
    "ensemble_op = EnsembleOperator(lm_modules=[g4o_mod, g4omini_mod, go1_mod])\n",
    "\n",
    "# 4) Instantiate a \"Judge\" operator (GetAnswerOperator or MostCommonOperator)\n",
    "#    Here let's assume \"GetAnswerOperator\" uses a 'final_judge' LMModule\n",
    "#judge_mod = LMModule(LMModuleConfig(model_name=\"o1-mini\"), model_service) # I don't think there's an o1-mini\n",
    "judge_mod = LMModule(LMModuleConfig(model_name=\"o1\"), model_service) # I don't think there's an o1-mini\n",
    "judge_op = GetAnswerOperator(lm_modules=[judge_mod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe37019-2949-4002-b7f2-9b5998af85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final answer from the judge: Setting up Avior for multi-model parallel usage involves configuring a system that allows multiple machine learning models to be deployed and executed in parallel efficiently. Here’s a general guide to help you set up such an environment:\n",
      "\n",
      "### Step 1: Understand Your Requirements\n",
      "- **Identify Models**: Determine which models you want to run in parallel and understand their resource requirements.\n",
      "- **Concurrency Level**: Decide how many models need to be executed simultaneously based on your workload.\n",
      "\n",
      "### Step 2: Define the Infrastructure\n",
      "- **Hardware Setup**: Ensure you have sufficient computational resources such as CPUs, GPUs, and memory to handle multiple models concurrently.\n",
      "- **Networking**: Set up a network architecture that supports fast data transfer between nodes, if models are distributed across different machines.\n",
      "\n",
      "### Step 3: Choose a Framework or Platform\n",
      "- **Containerization**: Use Docker or Kubernetes to containerize your models. This abstracts the underlying hardware and ensures that each model runs in its isolated environment.\n",
      "- **Job Scheduler**: Utilize job scheduling software like Apache Airflow or Kubernetes Jobs to manage and coordinate the parallel execution of various models.\n",
      "\n",
      "### Step 4: Configuration and Deployment\n",
      "- **Environment Configuration**: Configure each model with the required software dependencies and environment variables. Use tools like Anaconda or virtual environments to manage dependencies.\n",
      "- **Scaling**: Configure horizontal scaling to add more instances when the demand increases. Ensure vertical scaling capabilities in case individual models need more resources.\n",
      "\n",
      "### Step 5: Data Management\n",
      "- **Data Pipeline**: Set up efficient data pipelines to feed data into models. Use batch or stream processing frameworks like Apache Kafka or Apache Beam for data movement and processing.\n",
      "- **Data Storage**: Ensure you have quick read/write access to your data store. Consider distributed storage solutions like AWS S3, Google Cloud Storage, or HDFS.\n",
      "\n",
      "### Step 6: Load Balancing and Optimization\n",
      "- **Load Balancer**: Implement a load-balancing solution to distribute incoming requests among model instances effectively.\n",
      "- **Resource Optimization**: Continuously monitor resource utilization and optimize model configurations for better performance and lower latency.\n",
      "\n",
      "### Step 7: Monitoring and Logging\n",
      "- **Monitoring Tools**: Deploy monitoring tools like Prometheus or Grafana to keep track of system performance, model output, and resource usage.\n",
      "- **Logging**: Use centralized logging solutions like ELK (Elasticsearch, Logstash, Kibana) stack to capture and analyze logs from all running models.\n",
      "\n",
      "### Step 8: Testing and Validation\n",
      "- **Stress Testing**: Conduct\n"
     ]
    }
   ],
   "source": [
    "from src.avior.core.graph_executor import NoNGraphData, GraphExecutorService\n",
    "\n",
    "graph_data = NoNGraphData()\n",
    "# Node: \"ensemble\"\n",
    "graph_data.add_node(\n",
    "    name=\"ensemble\", \n",
    "    operator=ensemble_op, \n",
    "    inputs=[]  # no prior node dependencies\n",
    ")\n",
    "# Node: \"judge\"\n",
    "graph_data.add_node(\n",
    "    name=\"judge\",\n",
    "    operator=judge_op,\n",
    "    inputs=[\"ensemble\"]  # feed ensemble output into judge\n",
    ")\n",
    "\n",
    "# 5) Provide the final input to the graph\n",
    "input_data = {\n",
    "        \"query\": \"Explain how to set up Avior for multi-model parallel usage\"\n",
    "}\n",
    "\n",
    "# 6) Execute the graph\n",
    "executor_service = GraphExecutorService()\n",
    "results = executor_service.run(graph_data=graph_data, input_data=input_data)\n",
    "\n",
    "print(\"Final answer from the judge:\", results[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fa2b4-fc11-42b9-a8cf-932e4d86c3ce",
   "metadata": {},
   "source": [
    "#### HaluEval LLM as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c99280a-6bcd-45c5-8dc4-910d7ba73ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.avior.core.graph_executor import NoNGraphData, GraphExecutorService\n",
    "from src.avior.registry.model.config import initialize_global_registry, AviorSettings\n",
    "from src.avior.registry.model.services.model_service import ModelService\n",
    "from src.avior.modules.lm_modules import LMModule, LMModuleConfig\n",
    "from src.avior.registry.operator.hallucination_operators import (\n",
    "    QAHallucinationOperator,\n",
    "    DialogueHallucinationOperator,\n",
    "    SummarizationHallucinationOperator\n",
    ")\n",
    "import os\n",
    "\n",
    "def setup_hallucination_detection():\n",
    "\n",
    "    #Explicitly set the path to your .env file\n",
    "    env_path = \"/Users/kunalagrawal/Desktop/Research/ember/.env\"\n",
    "    \n",
    "    # Initialize settings with explicit env file path\n",
    "    settings = AviorSettings(_env_file=env_path)\n",
    "    \n",
    "    # Set API keys before initialization\n",
    "    settings.openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    #settings.anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "    #settings.google_api_key = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "\n",
    "    usage_service = UsageService()\n",
    "\n",
    "    # Create a model service using the registry\n",
    "    model_service = ModelService(registry=registry, usage_service=usage_service)\n",
    "    \n",
    "    # 2) Create LMModules for each\n",
    "    # gemini_mod = LMModule(LMModuleConfig(model_name=\"gemini-1.5-pro\"), model_service) #TODO might be bug in README, registry instead of model_service\n",
    "    # claude_mod = LMModule(LMModuleConfig(model_name=\"claude-3.5-sonnet\"), model_service)\n",
    "    \n",
    "    # Just using OpenAI models for now\n",
    "    g4o_mod = LMModule(LMModuleConfig(model_name=\"gpt-4o\"), model_service)\n",
    "    g4omini_mod = LMModule(LMModuleConfig(model_name=\"gpt-4o-mini\"), model_service)\n",
    "    go1_mod = LMModule(LMModuleConfig(model_name=\"o1\"), model_service)\n",
    "\n",
    "    # Create LM modules for different models\n",
    "\n",
    "    # Create appropriate operator based on task\n",
    "    lm_modules = [g4o_mod, g4omini_mod, go1_mod]\n",
    "\n",
    "    #Hardcoding to qa task for now\n",
    "    operator = QAHallucinationOperator(lm_modules)\n",
    "    \n",
    "    # Create graph\n",
    "    graph_data = NoNGraphData()\n",
    "    graph_data.add_node(\n",
    "        name=\"detector\",\n",
    "        operator=operator,\n",
    "        inputs=[]\n",
    "    )\n",
    "\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2228e46b-8ef4-45ea-8a55-2c420849b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['B', 'B', 'B']\n",
      "\n",
      "QA Hallucination Check:\n",
      "Query: Knowledge: Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\n",
      "Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
      "Candidate Answer: First for Women was started first.. Is this candidate answer supported by the provided knowledge?\n",
      "Result: judgement='B'\n"
     ]
    }
   ],
   "source": [
    "def run_hallucination_detection(**kwargs):\n",
    "    graph_data = setup_hallucination_detection()\n",
    "    \n",
    "    # The query is already formatted in the HaluEval format from the input\n",
    "    query = kwargs.get(\"query\", \"\")\n",
    "\n",
    "    input_data = {\n",
    "        \"query\": kwargs.get(\"query\"),\n",
    "        \"choices\": kwargs.get(\"choices\")\n",
    "    }\n",
    "    \n",
    "    executor_service = GraphExecutorService()\n",
    "    results = executor_service.run(graph_data=graph_data, input_data=input_data)\n",
    "    \n",
    "    if \"detector\" not in results:\n",
    "        return results\n",
    "    return results[\"detector\"]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    qa_example = {\n",
    "        \"query\": \"Knowledge: Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nCandidate Answer: First for Women was started first.. Is this candidate answer supported by the provided knowledge?\",\n",
    "        \"choices\": {\n",
    "            \"A\": \"Not Hallucinated\",\n",
    "            \"B\": \"Hallucinated\"\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"correct_answer\": \"B\"\n",
    "        }\n",
    "    }\n",
    "    qa_result = run_hallucination_detection(**qa_example)\n",
    "    print(f\"\\nQA Hallucination Check:\")\n",
    "    print(f\"Query: {qa_example['query']}\")\n",
    "    print(f\"Result: {qa_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a5cfb02-2837-43db-aee2-d434b41f41ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading and preparing dataset: halueval\n",
      "INFO:root:[load_and_prepare] Starting process for dataset 'halueval' with source='pminervini/HaluEval', config='config_name='qa' split='data'', num_samples='3'.\n",
      "INFO:root:[load_and_prepare] Converting config -> a string (or None) that _load_data expects.\n",
      "INFO:root:[load_and_prepare] Resolved config is: 'qa'.\n",
      "INFO:root:[load_and_prepare] Loading data from source='pminervini/HaluEval' using resolved_config='qa'.\n",
      "INFO:src.avior.registry.dataset.base.loaders:Checking dataset existence on the Hub: pminervini/HaluEval\n",
      "INFO:src.avior.registry.dataset.base.loaders:Loading dataset: pminervini/HaluEval (config: qa)\n",
      "INFO:src.avior.registry.dataset.base.loaders:Successfully loaded dataset: pminervini/HaluEval (config: qa)\n",
      "INFO:src.avior.registry.dataset.registry.service:Dataset columns: DatasetDict({\n",
      "    data: Dataset({\n",
      "        features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "DEBUG:src.avior.registry.dataset.registry.service:Split 'data' columns: ['knowledge', 'question', 'right_answer', 'hallucinated_answer']\n",
      "INFO:root:[load_and_prepare] Data loaded successfully.\n",
      "INFO:root:[load_and_prepare] Loaded dataset details: type=<class 'datasets.dataset_dict.DatasetDict'>, size=1\n",
      "INFO:root:[load_and_prepare] Example record from loaded dataset: DatasetDict({\n",
      "    data: Dataset({\n",
      "        features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n",
      "INFO:root:[load_and_prepare] Determined that 'config_obj' is a BaseDatasetConfig subclass.\n",
      "INFO:root:[load_and_prepare] Selecting appropriate split, if requested.\n",
      "INFO:root:[load_and_prepare] Split selection completed.\n",
      "INFO:root:[load_and_prepare] Validating dataset structure.\n",
      "INFO:root:[load_and_prepare] Dataset structure validated successfully.\n",
      "INFO:root:[load_and_prepare] Validated data details: type=<class 'datasets.arrow_dataset.Dataset'>, size=10000\n",
      "INFO:root:[load_and_prepare] Example record from validated data: {'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\", 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'right_answer': \"Arthur's Magazine\", 'hallucinated_answer': 'First for Women was started first.'}\n",
      "INFO:root:[load_and_prepare] Applying transformations to the validated data.\n",
      "INFO:root:[load_and_prepare] Data transformations applied.\n",
      "INFO:root:[load_and_prepare] Transformed data details: type=<class 'datasets.arrow_dataset.Dataset'>, size=10000\n",
      "INFO:root:[load_and_prepare] Example record from transformed data: {'knowledge': \"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\", 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'right_answer': \"Arthur's Magazine\", 'hallucinated_answer': 'First for Women was started first.'}\n",
      "INFO:root:[load_and_prepare] Validating required keys in the transformed data.\n",
      "INFO:root:[load_and_prepare] Required keys validation completed.\n",
      "INFO:root:[load_and_prepare] Sampling the data if num_samples is provided.\n",
      "INFO:root:[load_and_prepare] Transformed data: Dataset({\n",
      "    features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "INFO:root:[load_and_prepare] Sampled data: Dataset({\n",
      "    features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer'],\n",
      "    num_rows: 3\n",
      "})\n",
      "INFO:root:[load_and_prepare] Data sampling complete. Number of records after sampling: 3.\n",
      "INFO:root:[load_and_prepare] Example record from sampled data: Dataset({\n",
      "    features: ['knowledge', 'question', 'right_answer', 'hallucinated_answer'],\n",
      "    num_rows: 3\n",
      "})\n",
      "INFO:root:[load_and_prepare] Preparing final DatasetEntry objects.\n",
      "INFO:root:[load_and_prepare] Final preparation complete. Generated 6 DatasetEntry objects.\n",
      "INFO:__main__:Received 6 prepared entries for 'halueval'.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['A', 'A', 'A']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #1:\n",
      "{'query': \"Knowledge: Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nCandidate Answer: Arthur's Magazine. Is this candidate answer supported by the provided knowledge?\", 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'A'}}\n",
      "Result: judgement='A'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['B', 'B', 'B']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #2:\n",
      "{'query': \"Knowledge: Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.First for Women is a woman's magazine published by Bauer Media Group in the USA.\\nQuestion: Which magazine was started first Arthur's Magazine or First for Women?\\nCandidate Answer: First for Women was started first.. Is this candidate answer supported by the provided knowledge?\", 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'B'}}\n",
      "Result: judgement='B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['A', 'A', 'A']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #3:\n",
      "{'query': 'Knowledge: The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.The Oberoi Group is a hotel company with its head office in Delhi.\\nQuestion: The Oberoi family is part of a hotel company that has a head office in what city?\\nCandidate Answer: Delhi. Is this candidate answer supported by the provided knowledge?', 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'A'}}\n",
      "Result: judgement='A'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['B', 'B', 'B']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #4:\n",
      "{'query': \"Knowledge: The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.The Oberoi Group is a hotel company with its head office in Delhi.\\nQuestion: The Oberoi family is part of a hotel company that has a head office in what city?\\nCandidate Answer: The Oberoi family's hotel company is based in Mumbai.. Is this candidate answer supported by the provided knowledge?\", 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'B'}}\n",
      "Result: judgement='B'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n",
      "INFO:GraphExecutorService:GraphExecutorService run invoked.\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['A', 'A', 'A']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #5:\n",
      "{'query': 'Knowledge: Allison Beth \"Allie\" Goertz (born March 2, 1991) is an American musician. Goertz is known for her satirical songs based on various pop culture topics. Her videos are posted on YouTube under the name of Cossbysweater.Milhouse Mussolini van Houten is a fictional character featured in the animated television series \"The Simpsons\", voiced by Pamela Hayden, and created by Matt Groening who named the character after President Richard Nixon\\'s middle name.\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\\nCandidate Answer: President Richard Nixon. Is this candidate answer supported by the provided knowledge?', 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'A'}}\n",
      "Result: judgement='A'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:src.avior.registry.model.provider_registry.openai.openai_provider:OpenAI forward() invoked\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:GraphExecutorService:Graph execution completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judgements:  ['B', 'B', 'B']\n",
      "\n",
      "QA Hallucination:\n",
      "[HaluEval] Entry #6:\n",
      "{'query': 'Knowledge: Allison Beth \"Allie\" Goertz (born March 2, 1991) is an American musician. Goertz is known for her satirical songs based on various pop culture topics. Her videos are posted on YouTube under the name of Cossbysweater.Milhouse Mussolini van Houten is a fictional character featured in the animated television series \"The Simpsons\", voiced by Pamela Hayden, and created by Matt Groening who named the character after President Richard Nixon\\'s middle name.\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\\nCandidate Answer: Allie Goertz wrote a song about Milhouse, a popular TV character, named after an influential political figure.. Is this candidate answer supported by the provided knowledge?', 'choices': {'A': 'Not Hallucinated', 'B': 'Hallucinated'}, 'metadata': {'correct_answer': 'B'}}\n",
      "Result: judgement='B'\n"
     ]
    }
   ],
   "source": [
    "# Main Evaluation Logic\n",
    "import pandas as pd\n",
    "halu_df = pd.DataFrame(columns=['query','judgement','correct_answer'])\n",
    "\n",
    "\n",
    "# 9) Let's do the same for HaluEval:\n",
    "halu_info: Optional[DatasetInfo] = metadata_registry.get(\"halueval\")\n",
    "if not halu_info:\n",
    "    raise ValueError(\"HaluEval dataset not properly registered.\")\n",
    "\n",
    "halu_prepper_class = loader_factory.get_prepper_class(\"halueval\")\n",
    "if not halu_prepper_class:\n",
    "    raise ValueError(\"No HaluEval prepper found. Make sure it's registered.\")\n",
    "\n",
    "# Create config & prepper, defaulting to config_name=\"qa\", split=\"data\"\n",
    "halu_config = HaluEvalConfig()\n",
    "halu_prepper: IDatasetPrepper = halu_prepper_class(config=halu_config)\n",
    "\n",
    "logger.info(f\"Loading and preparing dataset: {halu_info.name}\")\n",
    "try:\n",
    "    halu_dataset_entries: List[DatasetEntry] = dataset_service.load_and_prepare(\n",
    "        dataset_info=halu_info,\n",
    "        prepper=halu_prepper,\n",
    "        config=halu_config,\n",
    "        num_samples=3\n",
    "    )\n",
    "    logger.info(f\"Received {len(halu_dataset_entries)} prepared entries for '{halu_info.name}'.\")\n",
    "    for i, entry in enumerate(halu_dataset_entries):\n",
    "        data_entry = entry.model_dump()\n",
    "        result = run_hallucination_detection(**data_entry)\n",
    "        print(f\"\\nQA Hallucination:\")\n",
    "        print(f\"[HaluEval] Entry #{i+1}:\\n{data_entry}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        new_row = {\"query\": data_entry['query'], \"judgement\": result.judgement, \"correct_answer\": data_entry['metadata']['correct_answer']} \n",
    "        halu_df = pd.concat([halu_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during HaluEval dataset preparation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c263865-2485-4477-b5e8-e924098ba016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>judgement</th>\n",
       "      <th>correct_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knowledge: Arthur's Magazine (1844–1846) was a...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Knowledge: Arthur's Magazine (1844–1846) was a...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Knowledge: The Oberoi family is an Indian fami...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Knowledge: The Oberoi family is an Indian fami...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knowledge: Allison Beth \"Allie\" Goertz (born M...</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Knowledge: Allison Beth \"Allie\" Goertz (born M...</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query judgement correct_answer\n",
       "0  Knowledge: Arthur's Magazine (1844–1846) was a...         A              A\n",
       "1  Knowledge: Arthur's Magazine (1844–1846) was a...         B              B\n",
       "2  Knowledge: The Oberoi family is an Indian fami...         A              A\n",
       "3  Knowledge: The Oberoi family is an Indian fami...         B              B\n",
       "4  Knowledge: Allison Beth \"Allie\" Goertz (born M...         A              A\n",
       "5  Knowledge: Allison Beth \"Allie\" Goertz (born M...         B              B"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "halu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565f4f4-dd75-41ea-910a-217411d95fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e4041-2ba4-4be2-8298-75a99e2ab2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
